# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- master

variables:
  databricks.notebook.path: /Users/wildwesthacker42@bpcs.com/notebooks
  databricks.cluster.name: hackathon-BestofAI
  databricks.cluster.id: 
  databricks.cluster.spark_version: 7.1.x-scala2.12
  databricks.cluster.node_type_id: Standard_DS3_v2
  databricks.cluster.driver_node_type_id: Standard_DS3_v2
  databricks.cluster.autotermination_minutes: 30
  databricks.cluster.workers.min: 2
  databricks.cluster.workers.max: 4
  databricks.job.train.name: '(BestofAI) - Train Machine Learning Model'
  databricks.job.train.id: 
  databricks.job.rulesbased.name: '(BestofAI) - Build & Apply Rules Based Model'
  databricks.job.rulesbased.id: 
  databricks.job.transitionmlmodel.name: '(BestofAI) - Transition ML Model to Production'
  databricks.job.transitionmlmodel.id: 
  databricks.job.transitionrulesbasedmodel.name: '(BestofAI) - Transition Rules Based Model to Production'
  databricks.job.transitionrulesbasedmodel.id: 

stages:
- stage: Build
  displayName: 'Create and deploy notebooks'
  jobs:
  - job: Train
    displayName: 'Create and deploy notebooks'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Set python 3.7'
      inputs:
        versionSpec: '3.7'
        addToPath: true
        architecture: 'x64'
    - task: Bash@3
      displayName: 'Install Databricks-Cli'
      inputs:
        targetType: 'inline'
        script: |
          # install databricks-cli
          pip install -U databricks-cli
        
    - task: Bash@3
      displayName: 'Configure Databricks CLI'
      inputs:
        targetType: 'inline'
        script: |
          # We need to write the pipe the conf into databricks configure --token since
          # that command only takes inputs from stdin. 
          conf=`cat << EOM
          $(databricks.host)
          $(databricks.token)
          EOM`
          
          # For password auth there are three lines expected
          # hostname, username, password
          echo "$conf" | databricks configure --token

    - task: Bash@3
      displayName: 'Create workspace folder'
      inputs:
        targetType: 'inline'
        script: databricks workspace mkdirs "$(databricks.notebook.path)"

    - task: Bash@3
      displayName: 'Import notebooks'
      inputs:
        targetType: 'inline'
        script: |
          # import notebooks
          databricks workspace import_dir -o notebooks "$(databricks.notebook.path)"

    - task: Bash@3
      displayName: 'Create / Get Cluster'
      inputs:
        targetType: 'inline'
        script: |
          cluster_id=$(databricks clusters list | grep "$(databricks.cluster.name)" | awk '{print $1}')
          
          if [ -z "$cluster_id" ]
          then
          JSON=`cat << EOM
          {
            "cluster_name": "$(databricks.cluster.name)",
            "spark_version": "$(databricks.cluster.spark_version)",
            "spark_conf": {
              "spark.databricks.delta.preview.enabled": "true"
            },
            "node_type_id": "$(databricks.cluster.node_type_id)",
            "driver_node_type_id": "$(databricks.cluster.driver_node_type_id)",
            "spark_env_vars": {
              "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
            },
            "autotermination_minutes": $(databricks.cluster.autotermination_minutes),
            "enable_elastic_disk": true,
            "autoscale": {
              "min_workers": $(databricks.cluster.workers.min),
              "max_workers": $(databricks.cluster.workers.max)
            },
            "init_scripts_safe_mode": false
          }
          EOM`
          
          cluster_id=$(databricks clusters create --json "$JSON" | jq -r ".cluster_id")
          sleep 10
          fi
          
          echo "##vso[task.setvariable variable=databricks.cluster.id;]$cluster_id"

    - task: Bash@3
      displayName: 'Start Cluster'
      inputs:
        targetType: 'inline'
        script: |
          echo "Checking Cluster State (Cluster ID: $(databricks.cluster.id))..."
          cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
          echo "Cluster State: $cluster_state"
          
          if [ $cluster_state == "TERMINATED" ]
          then
            echo "Starting Databricks Cluster..."
            databricks clusters start --cluster-id "$(databricks.cluster.id)"
            sleep 30
            cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
            echo "Cluster State: $cluster_state"
          fi
          
          while [ $cluster_state == "PENDING" ]
          do
            sleep 30
            cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
            echo "Cluster State: $cluster_state"
          done
          
          if [ $cluster_state == "RUNNING" ]
          then
            exit 0
          else
            exit 1
          fi

- stage: BuildRulesBased
  displayName: 'Build Rules Based Model'
  jobs:
  - job: BuildRulesBased
    displayName: 'Train, Evaluate & Register Model'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Set python 3.7'
      inputs:
        versionSpec: '3.7'
        addToPath: true
        architecture: 'x64'
    - task: Bash@3
      displayName: 'Install Databricks-Cli'
      inputs:
        targetType: 'inline'
        script: |
          # install databricks-cli
          pip install -U databricks-cli
        
    - task: Bash@3
      displayName: 'Configure Databricks CLI'
      inputs:
        targetType: 'inline'
        script: |
          # We need to write the pipe the conf into databricks configure --token since
          # that command only takes inputs from stdin. 
          conf=`cat << EOM
          $(databricks.host)
          $(databricks.token)
          EOM`
          
          # For password auth there are three lines expected
          # hostname, username, password
          echo "$conf" | databricks configure --token

    - task: Bash@3
      displayName: 'Create / Get Rules Based Model'
      inputs:
        targetType: 'inline'
        script: |
          job_id=$(databricks jobs list | grep "$(databricks.job.rulesbased.name)" | awk '{print $1}')
          
          if [ -z "$job_id" ]
          then
          echo "Creating $(databricks.job.rulesbased.name) job..."

          JSON=`cat << EOM
          {
            "existing_cluster_id": "$(databricks.cluster.id)",
            "name": "$(databricks.job.rulesbased.name)",
            "email_notifications": {},
            "timeout_seconds": 0,
            "notebook_task": {
              "notebook_path": "$(databricks.notebook.path)/Build_&_Apply_RulesBasedModel",
              "revision_timestamp": 0,
              "base_parameters": {
              }
            },
            "max_concurrent_runs": 1
          }
          EOM`
          
          job_id=$(databricks jobs create --json "$JSON" | jq ".job_id")
          fi
          
          echo "##vso[task.setvariable variable=databricks.job.rulesbased.id;]$job_id"

    - task: Bash@3
      displayName: 'Run Rules Based Model'
      inputs:
        targetType: 'inline'
        script: |
          echo "Running job with ID $(databricks.job.rulesbased.id) "
          run_id1=$(databricks jobs run-now --job-id $(databricks.job.rulesbased.id) | jq ".run_id")
          echo "  Run ID: $run_id1"
          run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
          echo "Run State (ID $run_id1): $run_state"
          while [ $run_state == "RUNNING" -o $run_state == "PENDING" ]
          do
            sleep 30
            run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
            echo "Run State (ID $run_id1): $run_state"
          done
          result_state1=$(databricks runs get --run-id $run_id1 | jq -r ".state.result_state")
          state_message1=$(databricks runs get --run-id $run_id1 | jq -r ".state.state_message")
          echo "Result State (ID $run_id1): $result_state1, Message: $state_message1"
          
          if [ $result_state1 == "SUCCESS" ]
          then
            exit 0
          else
            exit 1
          fi
        

- stage: TrainMLModel
  displayName: 'Train ML Model, Evaluate & Register Model'
  jobs:
  - job: TrainML
    displayName: 'Train, Evaluate & Register Model'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Set python 3.7'
      inputs:
        versionSpec: '3.7'
        addToPath: true
        architecture: 'x64'
    - task: Bash@3
      displayName: 'Install Databricks-Cli'
      inputs:
        targetType: 'inline'
        script: |
          # install databricks-cli
          pip install -U databricks-cli
        
    - task: Bash@3
      displayName: 'Configure Databricks CLI'
      inputs:
        targetType: 'inline'
        script: |
          # We need to write the pipe the conf into databricks configure --token since
          # that command only takes inputs from stdin. 
          conf=`cat << EOM
          $(databricks.host)
          $(databricks.token)
          EOM`
          
          # For password auth there are three lines expected
          # hostname, username, password
          echo "$conf" | databricks configure --token

    - task: Bash@3
      displayName: 'Create / Get Training Job'
      inputs:
        targetType: 'inline'
        script: |
          job_id=$(databricks jobs list | grep "$(databricks.job.train.name)" | awk '{print $1}')
          
          if [ -z "$job_id" ]
          then
          echo "Creating $(databricks.job.train.name) job..."

          JSON=`cat << EOM
          {
            "existing_cluster_id": "$(databricks.cluster.id)",
            "name": "$(databricks.job.train.name)",
            "email_notifications": {},
            "timeout_seconds": 0,
            "notebook_task": {
              "notebook_path": "$(databricks.notebook.path)/Train_Model",
              "revision_timestamp": 0,
              "base_parameters": {
              }
            },
            "max_concurrent_runs": 1
          }
          EOM`
          
          job_id=$(databricks jobs create --json "$JSON" | jq ".job_id")
          fi
          
          echo "##vso[task.setvariable variable=databricks.job.train.id;]$job_id"

    - task: Bash@3
      displayName: 'Run Training Job'
      inputs:
        targetType: 'inline'
        script: |
          echo "Running job with ID $(databricks.job.train.id) "
          run_id1=$(databricks jobs run-now --job-id $(databricks.job.train.id) | jq ".run_id")
          echo "  Run ID: $run_id1"
          run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
          echo "Run State (ID $run_id1): $run_state"
          while [ $run_state == "RUNNING" -o $run_state == "PENDING" ]
          do
            sleep 30
            run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
            echo "Run State (ID $run_id1): $run_state"
          done
          result_state1=$(databricks runs get --run-id $run_id1 | jq -r ".state.result_state")
          state_message1=$(databricks runs get --run-id $run_id1 | jq -r ".state.state_message")
          echo "Result State (ID $run_id1): $result_state1, Message: $state_message1"
          
          if [ $result_state1 == "SUCCESS" ]
          then
            exit 0
          else
            exit 1
          fi


- stage: TransitionModel
  displayName: 'Transition models to production'
  jobs:
  - job: TransitionModel
    displayName: 'Transition models to production'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Set python 3.7'
      inputs:
        versionSpec: '3.7'
        addToPath: true
        architecture: 'x64'
    - task: Bash@3
      displayName: 'Install Databricks-Cli'
      inputs:
        targetType: 'inline'
        script: |
          # install databricks-cli
          pip install -U databricks-cli
        
    - task: Bash@3
      displayName: 'Configure Databricks CLI'
      inputs:
        targetType: 'inline'
        script: |
          # We need to write the pipe the conf into databricks configure --token since
          # that command only takes inputs from stdin. 
          conf=`cat << EOM
          $(databricks.host)
          $(databricks.token)
          EOM`
          
          # For password auth there are three lines expected
          # hostname, username, password
          echo "$conf" | databricks configure --token

    - task: Bash@3
      displayName: 'Create / Get Transition of ML Job'
      inputs:
        targetType: 'inline'
        script: |
          job_id=$(databricks jobs list | grep "$(databricks.job.transitionmlmodel.name)" | awk '{print $1}')
          
          if [ -z "$job_id" ]
          then
          echo "Creating $(databricks.job.transitionmlmodel.name) job..."

          JSON=`cat << EOM
          {
            "existing_cluster_id": "$(databricks.cluster.id)",
            "name": "$(databricks.job.transitionmlmodel.name)",
            "email_notifications": {},
            "timeout_seconds": 0,
            "notebook_task": {
              "notebook_path": "$(databricks.notebook.path)/MLflow_transistion_model",
              "revision_timestamp": 0,
              "base_parameters": {
                "modelName": "CrossValidatorModel",
                "modelCurrentStage": "Staging",
                "modelNextStage": "Production"
              }
            },
            "max_concurrent_runs": 1
          }
          EOM`
          
          job_id=$(databricks jobs create --json "$JSON" | jq ".job_id")
          fi
          
          echo "##vso[task.setvariable variable=databricks.job.transitionmlmodel.id;]$job_id"

    - task: Bash@3
      displayName: 'Run Transition ML model to Production'
      inputs:
        targetType: 'inline'
        script: |
          echo "Running job with ID $(databricks.job.transitionmlmodel.id) "
          run_id1=$(databricks jobs run-now --job-id $(databricks.job.transitionmlmodel.id) | jq ".run_id")
          echo "  Run ID: $run_id1"
          run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
          echo "Run State (ID $run_id1): $run_state"
          while [ $run_state == "RUNNING" -o $run_state == "PENDING" ]
          do
            sleep 30
            run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
            echo "Run State (ID $run_id1): $run_state"
          done
          result_state1=$(databricks runs get --run-id $run_id1 | jq -r ".state.result_state")
          state_message1=$(databricks runs get --run-id $run_id1 | jq -r ".state.state_message")
          echo "Result State (ID $run_id1): $result_state1, Message: $state_message1"
          
          if [ $result_state1 == "SUCCESS" ]
          then
            exit 0
          else
            exit 1
          fi
    
    - task: Bash@3
      displayName: 'Create / Get Transition of Rules Based Model Job'
      inputs:
        targetType: 'inline'
        script: |
          job_id=$(databricks jobs list | grep "$(databricks.job.transitionrulesbasedmodel.name)" | awk '{print $1}')
          
          if [ -z "$job_id" ]
          then
          echo "Creating $(databricks.job.transitionrulesbasedmodel.name) job..."

          JSON=`cat << EOM
          {
            "existing_cluster_id": "$(databricks.cluster.id)",
            "name": "$(databricks.job.transitionrulesbasedmodel.name)",
            "email_notifications": {},
            "timeout_seconds": 0,
            "notebook_task": {
              "notebook_path": "$(databricks.notebook.path)/MLflow_transistion_model",
              "revision_timestamp": 0,
              "base_parameters": {
                "modelName": "RulesBasedModel",
                "modelCurrentStage": "Staging",
                "modelNextStage": "Production"
              }
            },
            "max_concurrent_runs": 1
          }
          EOM`
          
          job_id=$(databricks jobs create --json "$JSON" | jq ".job_id")
          fi
          
          echo "##vso[task.setvariable variable=databricks.job.transitionrulesbasedmodel.id;]$job_id"

    - task: Bash@3
      displayName: 'Run Transition Rules Based model to Production'
      inputs:
        targetType: 'inline'
        script: |
          echo "Running job with ID $(databricks.job.transitionrulesbasedmodel.id) "
          run_id1=$(databricks jobs run-now --job-id $(databricks.job.transitionrulesbasedmodel.id) | jq ".run_id")
          echo "  Run ID: $run_id1"
          run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
          echo "Run State (ID $run_id1): $run_state"
          while [ $run_state == "RUNNING" -o $run_state == "PENDING" ]
          do
            sleep 30
            run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
            echo "Run State (ID $run_id1): $run_state"
          done
          result_state1=$(databricks runs get --run-id $run_id1 | jq -r ".state.result_state")
          state_message1=$(databricks runs get --run-id $run_id1 | jq -r ".state.state_message")
          echo "Result State (ID $run_id1): $result_state1, Message: $state_message1"
          
          if [ $result_state1 == "SUCCESS" ]
          then
            exit 0
          else
            exit 1
          fi
